digraph "classes" {
rankdir=LR
charset="utf-8"
"minitorch.tensor_functions.Add" [color="black", fontcolor="black", label=<{Add|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor, t2: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.All" [color="black", fontcolor="black", label=<{All|<br ALIGN="LEFT"/>|forward(ctx: Context, a: Tensor, dim: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.autodiff.Context" [color="black", fontcolor="black", label=<{Context|no_grad : bool<br ALIGN="LEFT"/>saved_tensors<br ALIGN="LEFT"/>saved_values : Tuple[Any, ...]<br ALIGN="LEFT"/>|save_for_backward(): None<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.fast_conv.Conv1dFun" [color="black", fontcolor="black", label=<{Conv1dFun|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, input: Tensor, weight: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.fast_conv.Conv2dFun" [color="black", fontcolor="black", label=<{Conv2dFun|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, input: Tensor, weight: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Copy" [color="black", fontcolor="black", label=<{Copy|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.cuda_conv.CudaConv2dFun" [color="black", fontcolor="black", label=<{CudaConv2dFun|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, input: Tensor, kernel: Tensor, stride, padding): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.cuda_ops.CudaOps" [color="black", fontcolor="black", label=<{CudaOps|cuda : bool<br ALIGN="LEFT"/>|map(fn: Callable[[float], float]): MapProto<br ALIGN="LEFT"/>matrix_multiply(a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/>reduce(fn: Callable[[float, float], float], start: float): Callable[[Tensor, int], Tensor]<br ALIGN="LEFT"/>zip(fn: Callable[[float, float], float]): Callable[[Tensor, Tensor], Tensor]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.EQ" [color="black", fontcolor="black", label=<{EQ|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Exp" [color="black", fontcolor="black", label=<{Exp|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.fast_ops.FastOps" [color="black", fontcolor="black", label=<{FastOps|<br ALIGN="LEFT"/>|map(fn: Callable[[float], float]): MapProto<br ALIGN="LEFT"/>matrix_multiply(a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/>reduce(fn: Callable[[float, float], float], start: float): Callable[[Tensor, int], Tensor]<br ALIGN="LEFT"/>zip(fn: Callable[[float, float], float]): Callable[[Tensor, Tensor], Tensor]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Function" [color="black", fontcolor="black", label=<{Function|<br ALIGN="LEFT"/>|apply(): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.datasets.Graph" [color="black", fontcolor="black", label=<{Graph|N : int<br ALIGN="LEFT"/>X : List[Tuple[float, float]]<br ALIGN="LEFT"/>y : List[int]<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"minitorch.tensor.History" [color="black", fontcolor="black", label=<{History|ctx : Optional[Context]<br ALIGN="LEFT"/>inputs : Sequence[Tensor]<br ALIGN="LEFT"/>last_fn : Optional[Type[Function]]<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"minitorch.tensor_functions.Inv" [color="black", fontcolor="black", label=<{Inv|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.IsClose" [color="black", fontcolor="black", label=<{IsClose|<br ALIGN="LEFT"/>|forward(ctx: Context, a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.LT" [color="black", fontcolor="black", label=<{LT|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Log" [color="black", fontcolor="black", label=<{Log|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_ops.MapProto" [color="black", fontcolor="black", label=<{MapProto|<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"minitorch.tensor_functions.MatMul" [color="black", fontcolor="black", label=<{MatMul|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor, t2: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.nn.Max" [color="black", fontcolor="black", label=<{Max|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, float]<br ALIGN="LEFT"/>forward(ctx: Context, input: Tensor, dim: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.module.Module" [color="black", fontcolor="black", label=<{Module|training : bool<br ALIGN="LEFT"/>|add_parameter(k: str, v: Any): Parameter<br ALIGN="LEFT"/>eval(): None<br ALIGN="LEFT"/>modules(): Sequence[Module]<br ALIGN="LEFT"/>named_parameters(): Sequence[Tuple[str, Parameter]]<br ALIGN="LEFT"/>parameters(): Sequence[Parameter]<br ALIGN="LEFT"/>train(): None<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Mul" [color="black", fontcolor="black", label=<{Mul|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, Tensor]<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Neg" [color="black", fontcolor="black", label=<{Neg|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.optim.Optimizer" [color="black", fontcolor="black", label=<{Optimizer|parameters : Sequence[Parameter]<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"minitorch.module.Parameter" [color="black", fontcolor="black", label=<{Parameter|name : Optional[str]<br ALIGN="LEFT"/>value : Any<br ALIGN="LEFT"/>|update(x: Any): None<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Permute" [color="black", fontcolor="black", label=<{Permute|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, float]<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor, order: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.ReLU" [color="black", fontcolor="black", label=<{ReLU|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.optim.SGD" [color="black", fontcolor="black", label=<{SGD|lr : float<br ALIGN="LEFT"/>|step(): None<br ALIGN="LEFT"/>zero_grad(): None<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Sigmoid" [color="black", fontcolor="black", label=<{Sigmoid|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tensor<br ALIGN="LEFT"/>forward(ctx: Context, t1: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_ops.SimpleOps" [color="black", fontcolor="black", label=<{SimpleOps|is_cuda : bool<br ALIGN="LEFT"/>|map(fn: Callable[[float], float]): MapProto<br ALIGN="LEFT"/><I>matrix_multiply</I>(a: 'Tensor', b: 'Tensor'): 'Tensor'<br ALIGN="LEFT"/>reduce(fn: Callable[[float, float], float], start: float): Callable[['Tensor', int], 'Tensor']<br ALIGN="LEFT"/>zip(fn: Callable[[float, float], float]): Callable[['Tensor', 'Tensor'], 'Tensor']<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.Sum" [color="black", fontcolor="black", label=<{Sum|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, float]<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor, dim: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor.Tensor" [color="black", fontcolor="black", label=<{Tensor|backend<br ALIGN="LEFT"/>dims<br ALIGN="LEFT"/>f : Optional[TensorBackend]<br ALIGN="LEFT"/>grad : Optional[Tensor]<br ALIGN="LEFT"/>history : Optional[History]<br ALIGN="LEFT"/>name : str<br ALIGN="LEFT"/>parents<br ALIGN="LEFT"/>shape<br ALIGN="LEFT"/>size<br ALIGN="LEFT"/>unique_id : int<br ALIGN="LEFT"/>|accumulate_derivative(x: Any): None<br ALIGN="LEFT"/>all(dim: Optional[int]): Tensor<br ALIGN="LEFT"/>backward(grad_output: Optional[Tensor]): None<br ALIGN="LEFT"/>chain_rule(d_output: Any): Iterable[Tuple[Variable, Any]]<br ALIGN="LEFT"/>contiguous(): Tensor<br ALIGN="LEFT"/>detach(): Tensor<br ALIGN="LEFT"/>exp(): Tensor<br ALIGN="LEFT"/>expand(other: Tensor): Tensor<br ALIGN="LEFT"/>flip(dims: Tuple[int])<br ALIGN="LEFT"/>is_close(y: Tensor): Tensor<br ALIGN="LEFT"/>is_constant(): bool<br ALIGN="LEFT"/>is_leaf(): bool<br ALIGN="LEFT"/>item(): float<br ALIGN="LEFT"/>log(): Tensor<br ALIGN="LEFT"/>make(storage: Union[Storage, List[float]], shape: UserShape, strides: Optional[UserStrides], backend: Optional[TensorBackend]): Tensor<br ALIGN="LEFT"/>mean(dim: Optional[int]): Tensor<br ALIGN="LEFT"/>permute(): Tensor<br ALIGN="LEFT"/>relu(): Tensor<br ALIGN="LEFT"/>requires_grad(): bool<br ALIGN="LEFT"/>requires_grad_(x: bool): None<br ALIGN="LEFT"/>sigmoid(): Tensor<br ALIGN="LEFT"/>sum(dim: Optional[int]): Tensor<br ALIGN="LEFT"/>to_numpy(): npt.NDArray[np.float64]<br ALIGN="LEFT"/>tuple()<br ALIGN="LEFT"/>view(): Tensor<br ALIGN="LEFT"/>zero_grad_(): None<br ALIGN="LEFT"/>zeros(shape: Optional[UserShape]): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_ops.TensorBackend" [color="black", fontcolor="black", label=<{TensorBackend|add_reduce<br ALIGN="LEFT"/>add_zip<br ALIGN="LEFT"/>cuda<br ALIGN="LEFT"/>eq_zip<br ALIGN="LEFT"/>exp_map<br ALIGN="LEFT"/>id_cmap<br ALIGN="LEFT"/>id_map<br ALIGN="LEFT"/>inv_back_zip<br ALIGN="LEFT"/>inv_map<br ALIGN="LEFT"/>is_close_zip<br ALIGN="LEFT"/>log_back_zip<br ALIGN="LEFT"/>log_map<br ALIGN="LEFT"/>lt_zip<br ALIGN="LEFT"/>matrix_multiply<br ALIGN="LEFT"/>mul_reduce<br ALIGN="LEFT"/>mul_zip<br ALIGN="LEFT"/>neg_map<br ALIGN="LEFT"/>relu_back_zip<br ALIGN="LEFT"/>relu_map<br ALIGN="LEFT"/>sigmoid_map<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"minitorch.tensor_data.TensorData" [color="black", fontcolor="black", label=<{TensorData|dims : int<br ALIGN="LEFT"/>shape : Sequence<br ALIGN="LEFT"/>size : int<br ALIGN="LEFT"/>strides : Sequence<br ALIGN="LEFT"/>|get(key: UserIndex): float<br ALIGN="LEFT"/>index(index: Union[int, UserIndex]): int<br ALIGN="LEFT"/>indices(): Iterable[UserIndex]<br ALIGN="LEFT"/>is_contiguous(): bool<br ALIGN="LEFT"/>permute(): TensorData<br ALIGN="LEFT"/>sample(): UserIndex<br ALIGN="LEFT"/>set(key: UserIndex, val: float): None<br ALIGN="LEFT"/>shape_broadcast(shape_a: UserShape, shape_b: UserShape): UserShape<br ALIGN="LEFT"/>to_cuda_(): None<br ALIGN="LEFT"/>to_string(): str<br ALIGN="LEFT"/>tuple(): Tuple[np.ndarray, np.ndarray, np.ndarray]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_ops.TensorOps" [color="black", fontcolor="black", label=<{TensorOps|cuda : bool<br ALIGN="LEFT"/>|<I>cmap</I>(fn: Callable[[float], float]): Callable[[Tensor, Tensor], Tensor]<br ALIGN="LEFT"/><I>map</I>(fn: Callable[[float], float]): MapProto<br ALIGN="LEFT"/><I>matrix_multiply</I>(a: Tensor, b: Tensor): Tensor<br ALIGN="LEFT"/><I>reduce</I>(fn: Callable[[float, float], float], start: float): Callable[[Tensor, int], Tensor]<br ALIGN="LEFT"/><I>zip</I>(fn: Callable[[float, float], float]): Callable[[Tensor, Tensor], Tensor]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.autodiff.Variable" [color="black", fontcolor="black", label=<{Variable|parents<br ALIGN="LEFT"/>unique_id<br ALIGN="LEFT"/>|<I>accumulate_derivative</I>(x: Any): None<br ALIGN="LEFT"/><I>chain_rule</I>(d_output: Any): Iterable[Tuple['Variable', Any]]<br ALIGN="LEFT"/><I>is_constant</I>(): bool<br ALIGN="LEFT"/><I>is_leaf</I>(): bool<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.tensor_functions.View" [color="black", fontcolor="black", label=<{View|<br ALIGN="LEFT"/>|backward(ctx: Context, grad_output: Tensor): Tuple[Tensor, float]<br ALIGN="LEFT"/>forward(ctx: Context, a: Tensor, shape: Tensor): Tensor<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"minitorch.cuda_conv.CudaConv2dFun" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.cuda_ops.CudaOps" -> "minitorch.tensor_ops.TensorOps" [arrowhead="empty", arrowtail="none"];
"minitorch.fast_conv.Conv1dFun" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.fast_conv.Conv2dFun" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.fast_ops.FastOps" -> "minitorch.tensor_ops.TensorOps" [arrowhead="empty", arrowtail="none"];
"minitorch.nn.Max" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.optim.SGD" -> "minitorch.optim.Optimizer" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Add" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.All" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Copy" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.EQ" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Exp" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Inv" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.IsClose" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.LT" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Log" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.MatMul" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Mul" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Neg" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Permute" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.ReLU" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Sigmoid" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.Sum" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_functions.View" -> "minitorch.tensor_functions.Function" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor_ops.SimpleOps" -> "minitorch.tensor_ops.TensorOps" [arrowhead="empty", arrowtail="none"];
"minitorch.tensor.History" -> "minitorch.tensor.Tensor" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="history", style="solid"];
"minitorch.tensor.Tensor" -> "minitorch.tensor.Tensor" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="grad", style="solid"];
"minitorch.tensor_data.TensorData" -> "minitorch.tensor.Tensor" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="_tensor", style="solid"];
"minitorch.tensor_ops.TensorBackend" -> "minitorch.tensor.Tensor" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="backend", style="solid"];
}
